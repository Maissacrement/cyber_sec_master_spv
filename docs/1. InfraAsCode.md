# CoursÂ : Transition vers lâ€™Infrastructure as Code (IaC)

> **Objectif**Â : acquÃ©rir une comprÃ©hension opÃ©rationnelle dâ€™un workflow completÂ IaC couvrantâ€¯: design/containÃ©risation (Docker &â€¯dockerâ€‘compose) â†’ tests &Â sÃ©curitÃ© (CI/CD GitHubÂ Actions) â†’ dÃ©ploiement (Ansible) â†’ orchestration (Kubernetes).

---

## 1Â Â· IntroductionÂ : pourquoi lâ€™IaCÂ ?
- **ProblÃ¨mes de lâ€™approche manuelle**Â : dÃ©rive de configuration, reproductibilitÃ© impossible, dÃ©lais.
- **BÃ©nÃ©fices IaC**Â : infra dÃ©crite comme du code â‡’ versionnÃ©e, peerâ€‘review, rollback. Automatisation, scalabilitÃ©, audit.
- **Outils &Â piliers**Â : GitÂ / DockerÂ / CIâ€‘CDÂ / AnsibleÂ / Kubernetes.

### Atelier Ã©clair
```bash
# Cloner le dÃ©pÃ´t Â«â€¯iacâ€‘workshopâ€¯Â»
git clone https://github.com/Maissacrement/cyber_sec_master_spv.git && cd cyber_sec_master_spv
```
ObjectifÂ : valider que tout le monde a Git &Â Docker installÃ©s ("Helloâ€‘World" container).

---

## 2Â Â· DockerÂ : des conteneurs Ã  lâ€™image
### 2.1Â Dockerfile pas Ã  pas
| Instruction | RÃ´le | Exemple |
|-------------|------|---------|
| `FROM` | Image de base | `FROM python:3.12-slim` |
| `RUN` | Commande Ã  la construction | `RUN apt-get update && apt-get install -y git` |
| `COPY` | Copier des artefacts | `COPY app/ /app/` |
| `ENTRYPOINT` | Commande de lancement | `ENTRYPOINT ["python","app.py"]` |

```dockerfile
# docker/Dockerfile
FROM python:3.12-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
ENTRYPOINT ["python","app.py"]
```

### 2.2Â dockerâ€‘composeâ€¯: architecture multiâ€‘services
```yaml
version: "3.9"
services:
  web:
    build: ./docker
    ports: ["8000:8000"]
    depends_on: [db]
  db:
    image: postgres:16
    environment:
      POSTGRES_PASSWORD: example
volumes: {}
networks: {}
```
`docker compose up -d` â‡’ tout lâ€™environnement sâ€™instancie.

---

### 2.3Â RÃ©seau DockerÂ (CIDR, bridge, etc.)

Docker isole les conteneursÂ dans des rÃ©seaux virtuels et attribue des adresses IP issues dâ€™un **CIDR** (Classless Interâ€‘Domain Routing) â€” notation qui combine lâ€™adresse rÃ©seau et le masque, ex.Â `172.17.0.0/16`.

| Mode rÃ©seau | Description | CIDR / Exemple par dÃ©faut | Cas dâ€™usage |
|-------------|-------------|---------------------------|-------------|
| **bridge** *(par dÃ©faut)* | CrÃ©e un pont virtuel (`docker0`) ; chaque conteneur reÃ§oit une IP privÃ©e routÃ©e via NAT vers lâ€™hÃ´te. | `172.17.0.0/16` | Dev local, isolation lÃ©gÃ¨re |
| **host** | Conteneur partage la pile rÃ©seau de lâ€™hÃ´te (pas dâ€™IP dÃ©diÃ©e). | n/a | Performances, Ã©coute ports <1024 |
| **none** | Pas dâ€™interface rÃ©seau autoÂ ; conteneur isolÃ©. | n/a | SÃ©curitÃ©, sandbox |
| **overlay** | RÃ©seau distribuÃ© pour un swarm / multiâ€‘hÃ´tes. | `10.0.0.0/24` (ex.) | Cluster, microâ€‘services |
| **macvlan** | Conteneurs ont leur MAC / IP du LAN physique. | dÃ©pend LAN | Applis nÃ©cessitant L2 (DHCP, broadcast) |

#### 2.3.1Â Sousâ€‘rÃ©seaux personnalisÃ©s
```bash
# CrÃ©er un rÃ©seau bridge dÃ©diÃ© sur 10.10.0.0/24
docker network create \
  --driver bridge \
  --subnet 10.10.0.0/24 \
  --gateway 10.10.0.1 \
  mynet

# Lancer un conteneur dedans
docker run -d --name web --network mynet nginx
```
`docker inspect mynet` montre la configuration (CIDR, gateway, containers attachÃ©s).

#### 2.3.2Â Publier des ports ğŸ”€
- **`-p 8080:80`** â‡’ mappe *port hÃ´teÂ 8080* vers *port conteneurÂ 80*.
- **`-p 127.0.0.1:3306:3306`** â‡’ nâ€™Ã©coute que sur localhost.

#### 2.3.3Â dockerâ€‘compose & networks
```yaml
services:
  app:
    build: .
    networks:
      - back
networks:
  back:
    driver: bridge
    ipam:
      config:
        - subnet: 10.20.0.0/24
```
Compose crÃ©e automatiquement un DNS interneÂ ; `app` peut joindre `db:5432` par le nom du service.

*Tips*Â :
- VÃ©rifier les conflits CIDR avec VPN/entreprise.
- Utiliser `--icc=false` et `--iptables` pour durcir lâ€™isolation.

#### 2.3.4Â RÃ©solution de noms de service (DNS interne)
Docker embarque un **serveur DNS interne** (Ã  lâ€™adresse `127.0.0.11` dans chaque conteneur) qui fournit la **dÃ©couverte de services**Â :

| Contexte | Nom Ã  utiliser | Exemple |
|----------|----------------|---------|
| **Bridge ou overlay userâ€‘defined** | *Nom du conteneur* ou *nom du service compose* | `curl http://web:8000` |
| MÃªme conteneur sur plusieurs rÃ©seaux | FQDNÂ `<container>.<network>` | `ping db.mynet` |
| Alias dÃ©finis dans compose | Valeur dans `aliases:` | `redis-cache` |

> Les rÃ©seaux crÃ©Ã©s par `docker-compose` ajoutent automatiquement un domaine de rechercheÂ : `<project>_default`. Ainsi le service `db` devient rÃ©solvable via `db` ou `db.<project>_default`.

```yaml
services:
  web:
    build: .
    networks:
      back:
        aliases: [api.local]
  db:
    image: postgres
networks:
  back:
    driver: bridge
```
Dans le conteneur `web`Â :
```bash
# RÃ©solution automatique par le DNS Docker
ping db          # â†ª 10.20.0.2
curl http://api.local:8000
```

**Importantâ€¯:** la rÃ©solution ne fonctionne *que* pour les conteneurs connectÃ©s au *mÃªme rÃ©seau utilisateur* (bridge/overlay). Les conteneurs sur le rÃ©seau par dÃ©faut `bridge` et un rÃ©seau personnalisÃ© ne se voient pas sans y Ãªtre connectÃ©s tous les deux.

Pour diagnostiquerÂ :
```bash
# Lister les entrÃ©es DNS vues par le conteneur
cat /etc/resolv.conf
# Tester
getent hosts db
```

---
- VÃ©rifier les conflits CIDR avec VPN/entreprise.
- Utiliser `--icc=false` et `--iptables` pour durcir lâ€™isolation.

---

## 3Â Â· CIÂ /Â CD avec GitHubÂ Actions
### 3.1Â Pipeline minimal
```.github/workflows/ci.yml
name: Docker Image CI

on:
  push:
    branches: [ "main" ] # l'action s'execute a chaque push sur main
  pull_request:
    branches: [ "main" ] # l'action s'execute a chaque pull request sur main

jobs:

  build:

    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3
    - name: Made SAST docker running catainer analysis
      run: make workflow-attack-example running-container-analisis stop-workflow-attack-example # la commande est sensÃ© marchÃ© pour tout utilisateur present dans la racine du projet
    - name: Made SAST docker-compose file analysis
      run: make workflow-attack-example run-project-analysis stop-workflow-attack-example # la commande est sensÃ© marchÃ© pour tout utilisateur present dans la racine du projet
    - name: Try to compile attacker
      run: docker build . --file ./docker/Dockerfile.attack --tag attacker:$(date +%s) # On construit l'image pour savoir si elle, se construit de la meme faÃ§on sur une autre machine
```

#### 3.1.1Â Comprendre le runnerÂ : petite dÃ©mo *shell*
Un **runner GitHub** est une machine virtuelle Ã©phÃ©mÃ¨re (Ubuntuâ€¯22.04, Windows ou macOS) dÃ©marrÃ©e Ã  chaque workflow, **gratuite** pour les dÃ©pÃ´ts publics. Elle part dâ€™un disque propreÂ ; vos commandes sâ€™exÃ©cutent donc dans un environnement sain, idÃ©al pour vÃ©rifier que votre projet est vraiment reproducible.

Ajoutons une Ã©tapeÂ `ls` pour illustrerÂ :
```yaml
actions:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      # ... build et scans â€¦
      - name: Inspecter le workspace
        run: |
          echo "Contenu du rÃ©pertoire de travail (runner GitHub)Â :" 
          ls -lah
```
- **`runs-on: ubuntu-latest`**â€¯: rÃ©serve un VM Linux dans le cloud.
- **`run:`**â€¯: exÃ©cute des commandes shell directement dans cet hÃ´te.
- Lâ€™Ã©tapeÂ `checkout` clone votre dÃ©pÃ´tâ€¯; lâ€™instruction `ls` prouve que le code est bien prÃ©sent et que vous nâ€™Ãªtes *pas* sur votre poste local.

> GrÃ¢ce Ã  cet environnement immuable, les tests, lâ€™analyse statique et la construction dâ€™images se dÃ©roulent toujours dans les mÃªmes conditions âœ **qualitÃ© + versionning fiables**.

### 3.2Â StratÃ©gie de branches
`main` protÃ©gÃ©Â ; `feature/*` â‡’ PR + checks verts obligatoires.

---

## 4 Â· Ansible â€“ dÃ©ploiement automatisÃ©

### 4.1 Pourquoi Ansibleâ€¯?
Ansible est un **outil dâ€™automatisation** agentâ€‘lessâ€¯: pas de dÃ©mon Ã  installer sur les machines cibles, une simple connexion SSH (ou `local`) suffit. Il dÃ©crit lâ€™Ã©tat dÃ©sirÃ© dans des **playbooks** YAML et garantit lâ€™idempotence â€“ rejouable sans effets de bord.

### 4.2 Concepts clÃ©s
| Terme | RÃ´le | Exemple |
|-------|------|---------|
| **Inventaire** | Liste dâ€™hÃ´tes ou groupes | `localhost`, `webservers` |
| **Module** | UnitÃ© dâ€™action (apt, copy, docker_composeâ€¦) | `apt`, `community.docker.docker_compose` |
| **Playbook** | ScÃ©nario regroupant tÃ¢ches & variables | `deploy/local.yml` |
| **RÃ´le** | Collection rÃ©utilisable de tÃ¢ches/handlers | `geerlingguy.postgresql` |

### 4.3 Utiliser Ansible en **local**
```yaml
# playbooks/setup_local.yml
- name: Provisionner localhost
  hosts: localhost
  connection: local
  gather_facts: false
  tasks:
    - name: Installer htop
      become: yes
      apt:
        name: htop
        state: present

    - name: ExÃ©cuter un ls pour vÃ©rification
      shell: |
        ls -lah
        uname -a
```

ExÃ©cutionâ€¯:
```bash
ansible-playbook playbooks/setup_local.yml
```

### 4.4 DÃ©ploiement Docker avec `community.docker.docker_compose`
```yaml
# deploy/compose.yml
- name: DÃ©ployer les conteneurs de lâ€™application
  hosts: localhost
  connection: local
  gather_facts: false
  tasks:
    - name: Lancer lâ€™environnement compose
      community.docker.docker_compose:
        project_src: ../
        state: present
```

### 4.5 SÃ©curisation de base
```yaml
- name: SÃ©curiser la machine
  hosts: localhost
  become: yes
  tasks:
    - name: Mettre Ã  jour les paquets
      apt:
        update_cache: yes
        upgrade: dist

    - name: Installer Fail2ban
      apt:
        name: fail2ban
        state: present
```

### 4.6 IntÃ©gration dans la CI / CD
Dans GitHub Actions, un job `ansible` peut exÃ©cuterÂ :
```yaml
- uses: actions/setup-python@v5
- name: Install Ansible
  run: pip install ansible ansible-lint
- name: Run playbook
  run: ansible-playbook deploy/compose.yml -e "@vars/prod.yml"
```
Lint automatiqueÂ :
```bash
ansible-lint playbooks/  # dÃ©clenchÃ© dans la pipeline
```

---
## 5 Â· KubernetesÂ Â· Kubernetes
### 5.1Â Pourquoi migrerÂ ?
Limites de dockerâ€‘composeÂ : scaling, autoâ€‘healing, rolling update.

### 5.2Â Minikube local
```bash
minikube start --driver=docker
```

### 5.3Â DÃ©ployer Â«Â webÂ Â» + Postgres
`k8s/deployment.yaml`, `k8s/service.yaml`, `k8s/ingress.yaml` (avec Ingressâ€‘NGINX).

---

## 6Â Â· ChaÃ®ne complÃ¨teÂ : de lâ€™IDE Ã  la prod
1. DÃ©velopper âœ commit/push.
2. CIÂ : build + tests + scan.
3. CDÂ : tag Â«â€¯v*â€¯Â» dÃ©clenche job Ansible `--limit=prod`.
4. KubernetesÂ : rolling update.

DiagrammeÂ :
```mermaid
sequenceDiagram
Developer->>GitHub: push
GitHub->>CI: build & test
CI->>Registry: image v1.0
CI->>Ansible: playbook prod
Ansible->>K8s Cluster: apply manifests
```

---

## 7Â Â· Ressources
- Documentation officielleÂ : Docker, GitHub Actions, Ansible, Kubernetes.
- GuidesÂ SÃ©curitÃ©Â : CIS DockerÂ 1.5 benchmark, Snyk kube.
- OutilsÂ : Trivy, Hadolint, ansibleâ€‘lint, kubeâ€‘score.

---

> Fin du cours â€“ versionÂ 0.1
> *DerniÃ¨re mise Ã  jourÂ : 28Â avrilÂ 2025*



# Introduction Ã  Kubernetes

Lorsque les applications deviennent plus complexes (multi-conteneurs, besoin d'auto-scaling, haute disponibilitÃ©, etc.), **Kubernetes** s'impose naturellement comme une suite logique Ã  **docker-compose**.

**Kubernetes** est un orchestrateur de conteneurs qui :
- DÃ©ploie,
- GÃ¨re,
- Met Ã  lâ€™Ã©chelle,
- Et supervise vos applications conteneurisÃ©es.

En simplifiÃ© :
- **Docker-compose** dÃ©crit des **services** dans un fichier YAML.
- **Kubernetes** dÃ©crit des **dÃ©ploiements** et des **services**... Ã©galement en YAML, mais avec plus de puissance et de contrÃ´le.

| Concept docker-compose | Concept Kubernetes Ã©quivalent |
|:------------------------|:-------------------------------|
| `services:`             | `Deployment` + `Service`        |
| `volumes:`              | `PersistentVolumeClaim (PVC)`   |
| `networks:`             | `ClusterIP / NodePort / Ingress` |

Ainsi, migrer de **docker-compose** Ã  **Kubernetes** reste naturel, mÃªme si Kubernetes apporte plus de fonctionnalitÃ©s avancÃ©es.

---

# Minikube : Kubernetes local

Pour tester Kubernetes localement sans infrastructure lourde, **Minikube** est la solution idÃ©ale.

Minikube :
- Lance un **cluster Kubernetes complet** sur votre machine, dans une VM ou un conteneur Docker.
- Permet de dÃ©velopper et tester des applications Kubernetes **localement**, comme si vous Ã©tiez sur un cloud public.

**Installation rapide de Minikube :**
```bash
# Sur Linux/Mac
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo install minikube-linux-amd64 /usr/local/bin/minikube

# DÃ©marrer un cluster
minikube start
```

---

# DÃ©ployer et exposer un service sur Kubernetes

Le minimum pour dÃ©ployer une application sur Kubernetes est :

1. **Un fichier Deployment YAML** pour dÃ©crire comment lancer vos conteneurs :
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-container
        image: myimage:latest
        ports:
        - containerPort: 80
```

2. **Un fichier Service YAML** pour rendre votre application accessible :
```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-app-service
spec:
  type: NodePort
  selector:
    app: my-app
  ports:
  - port: 80
    targetPort: 80
    nodePort: 30080
```

Une fois appliquÃ© (`kubectl apply -f .`), vous pourrez accÃ©der Ã  votre service via :
```bash
minikube service my-app-service
```
Cela ouvre automatiquement lâ€™URL de votre application dans le navigateur.

---


